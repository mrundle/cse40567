{
 "metadata": {
  "name": "mrundle_assignment7"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": "Assignment 7"
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "**Matt Rundle**"
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "Note: I worked with classmate Andres Martin on this assignment."
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": "Define the Processing Functions "
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "This python implementation of association rule discovery and Apriori algorithm implementation was borrowed from https://github.com/cse40647/cse40647/blob/fa.14/10%20-%20Apriori.ipynb. This is what will do the processing on the dataset.\n\nLet it be also noted that many uses of these functions in this homework submission (including syntax and variable naming) was also inspired by the ipython notebook linked to above."
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "# (c) 2014 Everaldo Aguiar & Reid Johnson\n#\n# Modified from:\n# Marcel Caraciolo (https://gist.github.com/marcelcaraciolo/1423287)\n#\n# Functions to compute and extract association rules from a given frequent \n# itemset generated by the Apriori algorithm.\n\ndef apriori(dataset, min_support=0.5, VERBOSE=False):\n    \"\"\"Implements the Apriori algorithm.\n\n    The Apriori algorithm will iteratively generate new candidate \n    k-itemsets using the frequent (k-1)-itemsets found in the previous \n    iteration.\n\n    Parameters\n    ----------\n    dataset : list\n        The dataset (a list of transactions) from which to generate \n        candidate itemsets.\n\n    min_support : float\n        The minimum support threshold. Defaults to 0.5.\n\n    Returns\n    -------\n    F : list\n        The list of frequent itemsets.\n\n    support_data : dict\n        The support data for all candidate itemsets.\n\n    References\n    ----------\n    .. [1] R. Agrawal, R. Srikant, \"Fast Algorithms for Mining Association \n           Rules\", 1994.\n\n    \"\"\"\n    C1 = create_candidates(dataset)\n    D = map(set, dataset)\n    F1, support_data = support_prune(D, C1, min_support, VERBOSE=False) # prune candidate 1-itemsets\n    F = [F1] # list of frequent itemsets; initialized to frequent 1-itemsets\n    k = 2 # the itemset cardinality\n    while (len(F[k - 2]) > 0):\n        Ck = apriori_gen(F[k-2], k) # generate candidate itemsets\n        Fk, supK = support_prune(D, Ck, min_support) # prune candidate itemsets\n        support_data.update(supK) # update the support counts to reflect pruning\n        F.append(Fk) # add the pruned candidate itemsets to the list of frequent itemsets\n        k += 1\n\n    if VERBOSE:\n        # Print a list of all the frequent itemsets.\n        for kset in F:\n            for item in kset:\n                print \"\" \\\n                    + \"{\" \\\n                    + \"\".join(str(i) + \", \" for i in iter(item)).rstrip(', ') \\\n                    + \"}\" \\\n                    + \":  sup = \" + str(round(support_data[item], 3))\n\n    return F, support_data\n\ndef create_candidates(dataset, VERBOSE=False):\n    \"\"\"Creates a list of candidate 1-itemsets from a list of transactions.\n\n    Parameters\n    ----------\n    dataset : list\n        The dataset (a list of transactions) from which to generate candidate \n        itemsets.\n\n    Returns\n    -------\n    The list of candidate itemsets (c1) passed as a frozenset (a set that is \n    immutable and hashable).\n    \"\"\"\n    c1 = [] # list of all items in the database of transactions\n    for transaction in dataset:\n        for item in transaction:\n            if not [item] in c1:\n                c1.append([item])\n    c1.sort()\n\n    if VERBOSE:\n        # Print a list of all the candidate items.\n        print \"\" \\\n            + \"{\" \\\n            + \"\".join(str(i[0]) + \", \" for i in iter(c1)).rstrip(', ') \\\n            + \"}\"\n\n    # Map c1 to a frozenset because it will be the key of a dictionary.\n    return map(frozenset, c1)\n\ndef support_prune(dataset, candidates, min_support, VERBOSE=False):\n    \"\"\"Returns all candidate itemsets that meet a minimum support threshold.\n\n    By the apriori principle, if an itemset is frequent, then all of its \n    subsets must also be frequent. As a result, we can perform support-based \n    pruning to systematically control the exponential growth of candidate \n    itemsets. Thus, itemsets that do not meet the minimum support level are \n    pruned from the input list of itemsets (dataset).\n\n    Parameters\n    ----------\n    dataset : list\n        The dataset (a list of transactions) from which to generate candidate \n        itemsets.\n\n    candidates : frozenset\n        The list of candidate itemsets.\n\n    min_support : float\n        The minimum support threshold.\n\n    Returns\n    -------\n    retlist : list\n        The list of frequent itemsets.\n\n    support_data : dict\n        The support data for all candidate itemsets.\n    \"\"\"\n    sscnt = {} # set for support counts\n    for tid in dataset:\n        for can in candidates:\n            if can.issubset(tid):\n                sscnt.setdefault(can, 0)\n                sscnt[can] += 1\n\n    num_items = float(len(dataset)) # total number of transactions in the dataset\n    retlist = [] # array for unpruned itemsets\n    support_data = {} # set for support data for corresponding itemsets\n    for key in sscnt:\n        # Calculate the support of itemset key.\n        support = sscnt[key] / num_items\n        if support >= min_support:\n            retlist.insert(0, key)\n        support_data[key] = support\n\n    # Print a list of the pruned itemsets.\n    if VERBOSE:\n        for kset in retlist:\n            for item in kset:\n                print \"{\" + str(item) + \"}\"\n        print\n        for key in sscnt:\n            print \"\" \\\n                + \"{\" \\\n                + \"\".join([str(i) + \", \" for i in iter(key)]).rstrip(', ') \\\n                + \"}\" \\\n                + \":  sup = \" + str(support_data[key])\n\n    return retlist, support_data\n\ndef apriori_gen(freq_sets, k):\n    \"\"\"Generates candidate itemsets (via the F_k-1 x F_k-1 method).\n\n    This operation generates new candidate k-itemsets based on the frequent \n    (k-1)-itemsets found in the previous iteration. The candidate generation \n    procedure merges a pair of frequent (k-1)-itemsets only if their first k-2 \n    items are identical.\n\n    Parameters\n    ----------\n    freq_sets : list\n        The list of frequent (k-1)-itemsets.\n\n    k : integer\n        The cardinality of the current itemsets begin evaluated.\n\n    Returns\n    -------\n    retlist : list\n        The list of merged frequent itemsets.\n    \"\"\"\n    retList = [] # list of merged frequent itemsets\n    lenLk = len(freq_sets) # number of frequent itemsets\n    for i in range(lenLk):\n        for j in range(i+1, lenLk):\n            a=list(freq_sets[i])\n            b=list(freq_sets[j])\n            a.sort()\n            b.sort()\n            F1 = a[:k-2] # first k-2 items of freq_sets[i]\n            F2 = b[:k-2] # first k-2 items of freq_sets[j]\n\n            if F1 == F2: # if the first k-2 items are identical\n                # Merge the frequent itemsets.\n                retList.append(freq_sets[i] | freq_sets[j])\n\n    return retList\n\ndef rules_from_conseq(freq_set, H, support_data, rules, min_confidence=0.7):\n    \"\"\"Generates a set of candidate rules.\n\n    Parameters\n    ----------\n    freq_set : frozenset\n        The complete list of frequent itemsets.\n\n    H : list\n        A list of frequent itemsets (of a particular length).\n\n    support_data : dict\n        The support data for all candidate itemsets.\n\n    rules : list\n        A potentially incomplete set of candidate rules above the minimum \n        confidence threshold.\n\n    min_confidence : float\n        The minimum confidence threshold. Defaults to 0.7.\n    \"\"\"\n    m = len(H[0])\n    if (len(freq_set) > (m+1)):\n        Hmp1 = apriori_gen(H, m+1) # generate candidate itemsets\n        Hmp1 = calc_confidence(freq_set, Hmp1,  support_data, rules, min_confidence)\n        if len(Hmp1) > 1:\n            # If there are candidate rules above the minimum confidence \n            # threshold, recurse on the list of these candidate rules.\n            rules_from_conseq(freq_set, Hmp1, support_data, rules, min_confidence)\n\ndef calc_confidence(freq_set, H, support_data, rules, min_confidence=0.7, VERBOSE=False):\n    \"\"\"Evaluates the generated rules.\n\n    One measurement for quantifying the goodness of association rules is \n    confidence. The confidence for a rule 'P implies H' (P -> H) is defined as \n    the support for P and H divided by the support for P \n    (support (P|H) / support(P)), where the | symbol denotes the set union \n    (thus P|H means all the items in set P or in set H).\n\n    To calculate the confidence, we iterate through the frequent itemsets and \n    associated support data. For each frequent itemset, we divide the support \n    of the itemset by the support of the antecedent (left-hand-side of the \n    rule).\n\n    Parameters\n    ----------\n    freq_set : frozenset\n        The complete list of frequent itemsets.\n\n    H : list\n        A frequent itemset.\n\n    min_support : float\n        The minimum support threshold.\n\n    rules : list\n        A potentially incomplete set of candidate rules above the minimum \n        confidence threshold.\n\n    min_confidence : float\n        The minimum confidence threshold. Defaults to 0.7.\n\n    Returns\n    -------\n    pruned_H : list\n        The list of candidate rules above the minimum confidence threshold.\n    \"\"\"\n    pruned_H = [] # list of candidate rules above the minimum confidence threshold\n    for conseq in H: # iterate over the frequent itemsets\n        conf = support_data[freq_set] / support_data[freq_set - conseq]\n        if conf >= min_confidence:\n\n            rules.append((freq_set - conseq, conseq, conf))\n            pruned_H.append(conseq)\n\n            if VERBOSE:\n                print \"\" \\\n                    + \"{\" \\\n                    + \"\".join([str(i) + \", \" for i in iter(freq_set-conseq)]).rstrip(', ') \\\n                    + \"}\" \\\n                    + \" ---> \" \\\n                    + \"{\" \\\n                    + \"\".join([str(i) + \", \" for i in iter(conseq)]).rstrip(', ') \\\n                    + \"}\" \\\n                    + \":  conf = \" + str(round(conf, 3)) \\\n                    + \", sup = \" + str(round(support_data[freq_set], 3))\n\n    return pruned_H\n\ndef generate_rules(F, support_data, min_confidence=0.7, VERBOSE=False):\n    \"\"\"Generates a set of candidate rules from a list of frequent itemsets.\n\n    For each frequent itemset, we calculate the confidence of using a\n    particular item as the rule consequent (right-hand-side of the rule). By \n    testing and merging the remaining rules, we recursively create a list of \n    pruned rules.\n\n    Parameters\n    ----------\n    F : list\n        A list of frequent itemsets.\n\n    support_data : dict\n        The corresponding support data for the frequent itemsets (L).\n\n    min_confidence : float\n        The minimum confidence threshold. Defaults to 0.7.\n\n    Returns\n    -------\n    rules : list\n        The list of candidate rules above the minimum confidence threshold.\n    \"\"\"\n    rules = []\n    for i in range(1, len(F)):\n        for freq_set in F[i]:\n            H1 = [frozenset([item]) for item in freq_set]\n\n            if (i > 1):\n                rules_from_conseq(freq_set, H1, support_data, rules, min_confidence)\n            else:\n                calc_confidence(freq_set, H1, support_data, rules, min_confidence, VERBOSE=VERBOSE)\n\n    return rules",
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": "Define Minimum Support and Minimum Confidence"
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "MIN_SUPP = 0.05\nMIN_CONF = 0.5",
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": "Create Product Map"
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "# Create \nimport csv\nreader = csv.reader(open('/home/student/data_mining/hw7/bakery/products.csv'))\nproducts = {}\nfor row in reader:\n    pid, name = row\n    products[int(pid)] = name",
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": "Prcess the datasets"
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": "1000 elements"
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": "Read in the data"
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "T = [] # list of transactions\ninfile = open('/home/student/data_mining/hw7/bakery/1000/1000-out1.csv','r')\nfor line in infile:\n        # Append new transaction (skipping \\n and transaction number)\n        T.append(line.strip().split(',')[1:])\n\n# Map ID's to food names\nfor i in range (0,len(T)):\n    for j in range (0,len(T[i])):\n            T[i][j] = products[int(T[i][j])]",
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "# Generate candidate itemsets\n# This method isn't strictly necessary here (we could generate these some other way)\n# but we'll stick to the example.\nC1 = create_candidates(T, VERBOSE=False) # candidate 1-itemsets",
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": "Obtain Frequent Itemsets (1000 elements)"
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "# Prune candidate 1-itemsets via support-based pruning to generate frequent 1-itemsets.\nF1, support_data = support_prune(T, C1, MIN_SUPP, VERBOSE=False)\n\n# Generate all the frequent itemsets using the Apriori algorithm.\nF, support_data = apriori(T, MIN_SUPP, VERBOSE=True)",
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": "{Marzipan Cookie}:  sup = 0.09\n{Casino Cake}:  sup = 0.072\n{Apple Danish}:  sup = 0.084\n{Apple Tart}:  sup = 0.079\n{Tuile Cookie}:  sup = 0.102\n{Blackberry Tart}:  sup = 0.073\n{Chocolate Coffee}:  sup = 0.085\n{Strawberry Cake}:  sup = 0.091\n{Lemon Cookie}:  sup = 0.066\n{Green Tea}:  sup = 0.062\n{Berry Tart}:  sup = 0.095\n{Walnut Cookie}:  sup = 0.061\n{Apricot Croissant}:  sup = 0.076\n{Orange Juice}:  sup = 0.082\n{Raspberry Lemonade}:  sup = 0.072\n{Lemon Lemonade}:  sup = 0.066\n{Lemon Cake}:  sup = 0.085\n{Opera Cake}:  sup = 0.078\n{Chocolate Cake}:  sup = 0.084\n{Gongolais Cookie}:  sup = 0.108\n{Cherry Soda}:  sup = 0.077\n{Chocolate Tart}:  sup = 0.051\n{Lemon Tart}:  sup = 0.076\n{Apricot Tart}:  sup = 0.056\n{Cheese Croissant}:  sup = 0.078\n{Coffee Eclair}:  sup = 0.093\n{Almond Twist}:  sup = 0.065\n{Vanilla Frappuccino}:  sup = 0.074\n{Blueberry Tart}:  sup = 0.081\n{Truffle Cake}:  sup = 0.103\n{Apricot Danish}:  sup = 0.075\n{Raspberry Cookie}:  sup = 0.082\n{Hot Coffee}:  sup = 0.094\n{Single Espresso}:  sup = 0.059\n{Napoleon Cake}:  sup = 0.09\n{Bottled Water}:  sup = 0.077\n{Apple Croissant}:  sup = 0.091\n{Cherry Tart}:  sup = 0.084\n{Blueberry Danish}:  sup = 0.055\n{Apple Pie}:  sup = 0.068\n{Truffle Cake, Gongolais Cookie}:  sup = 0.058\n{Marzipan Cookie, Tuile Cookie}:  sup = 0.053\n"
      }
     ],
     "prompt_number": 7
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": "Association Rules (1000 elements)"
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "# Generate the association rules from a list of frequent itemsets.\nH = generate_rules(F, support_data, MIN_CONF, VERBOSE=True)\n\nprint \"\\nThere were \" + str(len(H)) + \" association rules meeting confidence threshold \" + str(MIN_CONF) + \" and support threshold \" + str(MIN_SUPP) + \".\\n\"",
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": "{Gongolais Cookie} ---> {Truffle Cake}:  conf = 0.537, sup = 0.058\n{Truffle Cake} ---> {Gongolais Cookie}:  conf = 0.563, sup = 0.058\n{Tuile Cookie} ---> {Marzipan Cookie}:  conf = 0.52, sup = 0.053\n{Marzipan Cookie} ---> {Tuile Cookie}:  conf = 0.589, sup = 0.053\n\nThere were 4 association rules meeting confidence threshold 0.5 and support threshold 0.05.\n\n"
      }
     ],
     "prompt_number": 8
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": "5000 elements"
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": "Read in the Data"
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "T2 = [] # list of transactions\ninfile = open('/home/student/data_mining/hw7/bakery/5000/5000-out1.csv','r')\nfor line in infile:\n        # Append new transaction (skipping \\n and transaction number)\n        T2.append(line.strip().split(',')[1:])\n\n# Map ID's to food names\nfor i in range (0,len(T2)):\n    for j in range (0,len(T2[i])):\n            T2[i][j] = products[int(T2[i][j])]",
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 9
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "# Generate candidate itemsets\n# This method isn't strictly necessary here (we could generate these some other way)\n# but we'll stick to the example.\nC2 = create_candidates(T2, VERBOSE=False) # candidate 1-itemsets",
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 10
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": "Obtain Frequent Itemsets (5000 elements)"
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "# Prune candidate 1-itemsets via support-based pruning to generate frequent 1-itemsets.\nF2, support_data_2 = support_prune(T2, C2, MIN_SUPP, VERBOSE=False)\n\n# Generate all the frequent itemsets using the Apriori algorithm.\nF2, support_data_2 = apriori(T2, MIN_SUPP, VERBOSE=True)",
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": "{Berry Tart}:  sup = 0.078\n{Marzipan Cookie}:  sup = 0.084\n{Casino Cake}:  sup = 0.075\n{Apple Danish}:  sup = 0.078\n{Apple Tart}:  sup = 0.073\n{Tuile Cookie}:  sup = 0.1\n{Blackberry Tart}:  sup = 0.076\n{Green Tea}:  sup = 0.062\n{Strawberry Cake}:  sup = 0.096\n{Lemon Cookie}:  sup = 0.064\n{Raspberry Lemonade}:  sup = 0.068\n{Chocolate Coffee}:  sup = 0.081\n{Lemon Cake}:  sup = 0.085\n{Walnut Cookie}:  sup = 0.071\n{Apricot Croissant}:  sup = 0.082\n{Orange Juice}:  sup = 0.092\n{Lemon Lemonade}:  sup = 0.065\n{Opera Cake}:  sup = 0.084\n{Chocolate Cake}:  sup = 0.08\n{Gongolais Cookie}:  sup = 0.095\n{Cherry Soda}:  sup = 0.067\n{Chocolate Tart}:  sup = 0.076\n{Lemon Tart}:  sup = 0.071\n{Cheese Croissant}:  sup = 0.077\n{Coffee Eclair}:  sup = 0.111\n{Almond Twist}:  sup = 0.081\n{Vanilla Frappuccino}:  sup = 0.073\n{Blueberry Tart}:  sup = 0.085\n{Truffle Cake}:  sup = 0.088\n{Apricot Danish}:  sup = 0.089\n{Raspberry Cookie}:  sup = 0.064\n{Hot Coffee}:  sup = 0.103\n{Single Espresso}:  sup = 0.065\n{Napoleon Cake}:  sup = 0.082\n{Bottled Water}:  sup = 0.077\n{Apple Croissant}:  sup = 0.074\n{Cherry Tart}:  sup = 0.092\n{Apple Pie}:  sup = 0.078\n{Cherry Tart, Apricot Danish}:  sup = 0.051\n"
      }
     ],
     "prompt_number": 11
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": "Association Rules (5000 elements)"
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "# Generate the association rules from a list of frequent itemsets.\nH2 = generate_rules(F2, support_data_2, MIN_CONF, VERBOSE=True)\n\nprint \"\\nThere were \" + str(len(H2)) + \" association rules meeting confidence threshold \" + str(MIN_CONF) + \" and support threshold \" + str(MIN_SUPP) + \".\\n\"",
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": "{Apricot Danish} ---> {Cherry Tart}:  conf = 0.574, sup = 0.051\n{Cherry Tart} ---> {Apricot Danish}:  conf = 0.557, sup = 0.051\n\nThere were 2 association rules meeting confidence threshold 0.5 and support threshold 0.05.\n\n"
      }
     ],
     "prompt_number": 12
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": "20,000 elements"
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": "Read in the Data"
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "T3 = [] # list of transactions\ninfile = open('/home/student/data_mining/hw7/bakery/20000/20000-out1.csv','r')\nfor line in infile:\n        # Append new transaction (skipping \\n and transaction number)\n        T3.append(line.strip().split(',')[1:])\n\n# Map ID's to food names\nfor i in range (0,len(T3)):\n    for j in range (0,len(T3[i])):\n            T3[i][j] = products[int(T3[i][j])]",
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 13
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "# Generate candidate itemsets\n# This method isn't strictly necessary here (we could generate these some other way)\n# but we'll stick to the example.\nC3 = create_candidates(T3, VERBOSE=False) # candidate 1-itemsets",
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 14
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": "Obtain Frequent Itemsets (20,000 elements)"
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "# Prune candidate 1-itemsets via support-based pruning to generate frequent 1-itemsets.\nF3, support_data_3 = support_prune(T3, C3, MIN_SUPP, VERBOSE=False)\n\n# Generate all the frequent itemsets using the Apriori algorithm.\nF3, support_data_3 = apriori(T3, MIN_SUPP, VERBOSE=True)",
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": "{Marzipan Cookie}:  sup = 0.086\n{Casino Cake}:  sup = 0.075\n{Apple Danish}:  sup = 0.068\n{Apple Tart}:  sup = 0.069\n{Tuile Cookie}:  sup = 0.099\n{Blackberry Tart}:  sup = 0.077\n{Chocolate Coffee}:  sup = 0.082\n{Strawberry Cake}:  sup = 0.092\n{Lemon Cookie}:  sup = 0.068\n{Green Tea}:  sup = 0.062\n{Berry Tart}:  sup = 0.084\n{Lemon Cake}:  sup = 0.086\n{Walnut Cookie}:  sup = 0.07\n{Apricot Croissant}:  sup = 0.082\n{Orange Juice}:  sup = 0.092\n{Lemon Lemonade}:  sup = 0.067\n{Opera Cake}:  sup = 0.084\n{Chocolate Cake}:  sup = 0.084\n{Gongolais Cookie}:  sup = 0.092\n{Cherry Soda}:  sup = 0.065\n{Chocolate Tart}:  sup = 0.076\n{Lemon Tart}:  sup = 0.076\n{Cheese Croissant}:  sup = 0.082\n{Coffee Eclair}:  sup = 0.11\n{Almond Twist}:  sup = 0.073\n{Vanilla Frappuccino}:  sup = 0.077\n{Raspberry Lemonade}:  sup = 0.068\n{Truffle Cake}:  sup = 0.085\n{Apricot Danish}:  sup = 0.093\n{Raspberry Cookie}:  sup = 0.069\n{Blueberry Tart}:  sup = 0.084\n{Hot Coffee}:  sup = 0.104\n{Single Espresso}:  sup = 0.071\n{Napoleon Cake}:  sup = 0.085\n{Bottled Water}:  sup = 0.072\n{Apple Croissant}:  sup = 0.071\n{Cherry Tart}:  sup = 0.091\n{Apple Pie}:  sup = 0.074\n{Cherry Tart, Apricot Danish}:  sup = 0.053\n"
      }
     ],
     "prompt_number": 15
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": "Association Rules (20,000 elements)"
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "# Generate the association rules from a list of frequent itemsets.\nH3 = generate_rules(F3, support_data_3, MIN_CONF, VERBOSE=True)\n\nprint \"\\nThere were \" + str(len(H3)) + \" association rules meeting confidence threshold \" + str(MIN_CONF) + \" and support threshold \" + str(MIN_SUPP) + \".\\n\"",
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": "{Apricot Danish} ---> {Cherry Tart}:  conf = 0.567, sup = 0.053\n{Cherry Tart} ---> {Apricot Danish}:  conf = 0.576, sup = 0.053\n\nThere were 2 association rules meeting confidence threshold 0.5 and support threshold 0.05.\n\n"
      }
     ],
     "prompt_number": 16
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": "75,000 transactions"
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": "Read in the Data"
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "T4 = [] # list of transactions\ninfile = open('/home/student/data_mining/hw7/bakery/75000/75000-out1.csv','r')\nfor line in infile:\n        # Append new transaction (skipping \\n and transaction number)\n        T4.append(line.strip().split(',')[1:])\n\n# Map ID's to food names\nfor i in range (0,len(T4)):\n    for j in range (0,len(T4[i])):\n            T4[i][j] = products[int(T4[i][j])]",
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 17
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "# Generate candidate itemsets\n# This method isn't strictly necessary here (we could generate these some other way)\n# but we'll stick to the example.\nC4 = create_candidates(T4, VERBOSE=False) # candidate 1-itemsets",
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 18
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": "Obtain Frequent Itemsets (75,000 elements)"
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "# Prune candidate 1-itemsets via support-based pruning to generate frequent 1-itemsets.\nF4, support_data_4 = support_prune(T4, C4, MIN_SUPP, VERBOSE=False)\n\n# Generate all the frequent itemsets using the Apriori algorithm.\nF4, support_data_4 = apriori(T4, MIN_SUPP, VERBOSE=True)",
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": "{Marzipan Cookie}:  sup = 0.09\n{Casino Cake}:  sup = 0.075\n{Apple Danish}:  sup = 0.068\n{Apple Tart}:  sup = 0.068\n{Tuile Cookie}:  sup = 0.101\n{Blackberry Tart}:  sup = 0.076\n{Chocolate Coffee}:  sup = 0.083\n{Vanilla Frappuccino}:  sup = 0.077\n{Green Tea}:  sup = 0.062\n{Apple Croissant}:  sup = 0.067\n{Berry Tart}:  sup = 0.085\n{Walnut Cookie}:  sup = 0.068\n{Apricot Croissant}:  sup = 0.084\n{Orange Juice}:  sup = 0.092\n{Raspberry Lemonade}:  sup = 0.068\n{Lemon Lemonade}:  sup = 0.068\n{Lemon Cake}:  sup = 0.084\n{Opera Cake}:  sup = 0.082\n{Chocolate Cake}:  sup = 0.084\n{Gongolais Cookie}:  sup = 0.09\n{Cherry Soda}:  sup = 0.062\n{Lemon Cookie}:  sup = 0.068\n{Lemon Tart}:  sup = 0.076\n{Cheese Croissant}:  sup = 0.082\n{Coffee Eclair}:  sup = 0.109\n{Almond Twist}:  sup = 0.077\n{Strawberry Cake}:  sup = 0.093\n{Blueberry Tart}:  sup = 0.083\n{Truffle Cake}:  sup = 0.082\n{Apricot Danish}:  sup = 0.093\n{Raspberry Cookie}:  sup = 0.068\n{Hot Coffee}:  sup = 0.103\n{Single Espresso}:  sup = 0.068\n{Napoleon Cake}:  sup = 0.083\n{Bottled Water}:  sup = 0.075\n{Chocolate Tart}:  sup = 0.074\n{Cherry Tart}:  sup = 0.093\n{Apple Pie}:  sup = 0.077\n{Cherry Tart, Apricot Danish}:  sup = 0.053\n{Marzipan Cookie, Tuile Cookie}:  sup = 0.051\n"
      }
     ],
     "prompt_number": 19
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": "Association Rules (75,000 elements)"
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "# Generate the association rules from a list of frequent itemsets.\nH4 = generate_rules(F4, support_data_4, MIN_CONF, VERBOSE=True)\nprint \"\\nThere were \" + str(len(H4)) + \" association rules meeting confidence threshold \" + str(MIN_CONF) + \" and support threshold \" + str(MIN_SUPP) + \".\\n\"",
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": "{Apricot Danish} ---> {Cherry Tart}:  conf = 0.574, sup = 0.053\n{Cherry Tart} ---> {Apricot Danish}:  conf = 0.57, sup = 0.053\n{Tuile Cookie} ---> {Marzipan Cookie}:  conf = 0.505, sup = 0.051\n{Marzipan Cookie} ---> {Tuile Cookie}:  conf = 0.567, sup = 0.051\n\nThere were 4 association rules meeting confidence threshold 0.5 and support threshold 0.05.\n\n"
      }
     ],
     "prompt_number": 20
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": "Analysis"
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "Here are the number of association rules found for each size of the dataset:"
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "print \"1000 : \" + str(len(H))\nprint \"5000 : \" + str(len(H2))\nprint \"20000: \" + str(len(H3))\nprint \"75000: \" + str(len(H4))",
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": "1000 : 4\n5000 : 2\n20000: 2\n75000: 4\n"
      }
     ],
     "prompt_number": 21
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": "Breakdown"
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "With the 1000 transaction dataset, none of the association rules that we found were supported with later iterations on larger datasets. This shows us that we will often see anomalies in association rules when processing a small amount of data. The 5000 and 2000 transaction datasets yielded the same association rules as eachother, with both sets at similar confidences and supports. The 75,000 transaction dataset yielded the same association rules as the last two (5000 and 2000,) with the addition of two more association rules. This shows us that with more data, we can often expose more association rule."
    }
   ],
   "metadata": {}
  }
 ]
}